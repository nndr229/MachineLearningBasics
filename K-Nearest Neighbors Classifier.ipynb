{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kNN** can classify examples by assigning them the class of similarly labeled examples.<br>\n",
    "**kNN** is well suited for classification tasks where the relationship between features are very hard to understand.<br>\n",
    "The **training dataset** contains examples that have been classifed into several categories.<br>\n",
    "We have a new example(with the same number of features as the training data). The **kNN** algorithm identifies **k** elements in the training dataset that are nearest in similarity.<br>\n",
    "The **unlabeled test example** is assigned the **class** of it's **nearest neighbors** that have similar features.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data in the table below.We have **features** foreach data entry.<br>\n",
    "Those features are Ingredient, Sweetness(Parameter) and Crunchiness(Parameter).<br>\n",
    "Parameters from 1-10.<br>\n",
    "Each entry also has a **label** (given by Food-Type).<br>\n",
    "Based on the data **kNN** will classify the data ino 3 classes -- **fruit, protein or vegetable**.<br>\n",
    "Any new entry after training will be classified into one of these 3 classes or will be an outlier.<br>\n",
    "Say, the new entry is **tomato*** with Sweetness **6** and Crunchiness **4**. Where will it fit???<br>\n",
    "We use a **distance function (like Euclidean Distance)** to classify the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr style=\"color:green\">\n",
    "            <td>Ingredient</td>\n",
    "            <td>Sweetness</td>\n",
    "            <td>Crunchiness</td>\n",
    "            <td>Food-Type</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Apple</td>\n",
    "            <td>10</td>\n",
    "            <td>9</td>\n",
    "            <td>fruit</td>\n",
    "        </tr>\n",
    "       <tr>\n",
    "            <td>Bacon</td>\n",
    "            <td>1</td>\n",
    "            <td>4</td>\n",
    "            <td>protein</td>\n",
    "        </tr>\n",
    "       <tr>\n",
    "            <td>Banana</td>\n",
    "            <td>10</td>\n",
    "            <td>1</td>\n",
    "            <td>fruit</td>\n",
    "        </tr>\n",
    "       <tr>\n",
    "            <td>Carrot</td>\n",
    "            <td>7</td>\n",
    "            <td>10</td>\n",
    "            <td>vegetable</td>\n",
    "        </tr>       \n",
    "        <tr>\n",
    "            <td>Deagon Fruit</td>\n",
    "            <td>1</td>\n",
    "            <td>1</td>\n",
    "            <td>fruit</td>\n",
    "        </tr>\n",
    "        <tr style=\"color:red\">\n",
    "            <td>Tomato</td>\n",
    "            <td>6</td>\n",
    "            <td>4</td>\n",
    "            <td>???</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy learner algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy learners **do not learn anything**!!!<br>\n",
    "We just store the training data.There is <strong><span style=\"color:red\">NO TRAINING AT ALL<span></strong>.<br>\n",
    "Making Predictions is rather slow as there are a lot of distances to calculate.<br>\n",
    "**WE DO NOT BIULD A MODEL**. This is an example of non-parametric learning.So no need to use an optimization algorithm.No $\\beta$ parameters.<br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Euclidean Distance** Between two points $P_1$ and $P_2$ is: <br><br> \n",
    "$dist(P_1,P_2) = \\sqrt{(x_{p_1} - x_{p_2})^2 + (y_{p_1} - y_{p_2})^2 + (z_{p_1} - z_{p_2})^2+ \\ ... \\ + (f_{p_1} - f_{p_2})^2 }$<br><br>\n",
    "Where $x_{p_1} \\ to \\ f_{p_1} $ represents all the dimensions of point $P_1$. Same goes for $P_2$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply this to our dataset to  <span style =\"color:red\">CLASSIFY TOMATO</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dist(tomato,carrot)\\ = \\sqrt{(6-7)^2+(4-10)^2} \\ \\ $ = **6.803**<br>\n",
    "$dist(tomato,apple) \\ \\ = \\sqrt{(6-10)^2+(4-9)^2}\\  \\ $   = **6.403**<br>\n",
    "$dist(tomato,bacon) \\ \\ = \\sqrt{(6-1)^2+(4-4)^2} \\ \\ \\ \\ $  = **5**<br>\n",
    "$dist(tomato,banana)  =  \\ \\sqrt{(6-10)^2+(4-1)^2} \\   $ = **5**<br>\n",
    "$dist(tomato,dragon fruit) \\ = \\sqrt{(6-1)^2+(4-1)^2}  $  = **5.88**<br><br>\n",
    "Now we have to consider the k-parameters -- <br>\n",
    "**If k=1** We consider the **smallest distance**  : Bacon and Banana.<br>\n",
    "**If k=2** We consider the **2 smallest distances** : Bacon and Banana. So there is 50% chance that tomato is a protein and 50% chance that tomato is a fruit.<br>\n",
    "**If k=3** We consider the **3 smallest distance** : Bacon,Banana and Dragon Fruit.Because tomato is closer to 2 fruits and 1 protien, It is a <span style=\"color:red\">FRUIT</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose this \"k\" value ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k** value will determine how  many neighbors to use for the classification. This will affect how the model will generalize and work on other datasets.<br>\n",
    "**k is small** -- Noisy data or outliers have a huge impact on our classifier. This is called **overfitting** (The model will have high variance and will be less biased).<br>\n",
    "**k is large** &nbsp;-- The classifer has a tendency to predict the majority class regardless of which neighbors are nearest. This is called **underfitting** (Variance is small but there will be high bias).<br>\n",
    "<strong><span style=\"color:red\">WE HAVE TO CHOOSE THE \"k\" VALUE CAREFULLY!!!</span></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:midnightblue\"> CODE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is split into two classes Blue and Red\n",
    "# xBlue and yBlue are the features of the Blue class represented as points in a 2D space\n",
    "xBlue = np.array([0.3,0.5,1,1.4,1.7,2])\n",
    "yBlue = np.array([1,4.5,2.3,1.9,8.9,4.1])\n",
    "\n",
    "# xRed and yRed are the features of the Red class represented as points in a 2D space\n",
    "xRed = np.array([3.3,3.5,4,4.4,5.7,6])\n",
    "yRed = np.array([7,1.5,6.3,1.9,2.9,7.1])\n",
    "\n",
    "#X below is the collection of all points in the dataset\n",
    "#y below if the the labels for each point in X\n",
    "X = np.array([[0.3,1],[0.5,4.5],[1,2.3],[1.4,1.9],[1.7,8.9],[2,4.1],[3.3,7],[3.5,1.5],[4,6.3],[4.4,1.9],[5.7,2.9],[6,7.1]])\n",
    "y = np.array([0,0,0,0,0,0,1,1,1,1,1,1]) # 0: blue class, 1: red class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets plot this out\n",
    "plt.plot(xBlue,yBlue,\"ro\",color=\"blue\")\n",
    "plt.plot(xRed,yRed,\"ro\",color=\"red\")\n",
    "\n",
    "#Point to classify\n",
    "plt.plot(3,5,\"ro\",color=\"green\",markersize=15)\n",
    "plt.axis([-0.5,10,-0.5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier's Prediction for a new point with features 3,5 (green point in our graph) is : [1] that is the red class.\n",
      "Classifier's Prediction for a new point with features 1,3  is : [0] that is the blue class.\n"
     ]
    }
   ],
   "source": [
    "#Let's classify the data\n",
    "classifier = KNeighborsClassifier(n_neighbors=3) #k-value\n",
    "classifier.fit(X,y)\n",
    "\n",
    "#Let's make a prediction\n",
    "pred_1 = classifier.predict(np.array([[3,5]]))\n",
    "pred_2 = classifier.predict(np.array([[1,3]]))\n",
    "print(f\"Classifier's Prediction for a new point with features 3,5 \"+ \n",
    "      f\"(green point in our graph) is : {pred_1} that is the red class.\")\n",
    "print(f\"Classifier's Prediction for a new point with features 1,3 \"+ \n",
    "      f\" is : {pred_2} that is the blue class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:midnightblue\"> Let's try out a serious example!<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the credit_data.csv file used for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing #for min-max transformation and z-tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          clientid        income          age          loan          LTI  \\\n",
      "count  2000.000000   2000.000000  2000.000000   2000.000000  2000.000000   \n",
      "mean   1000.500000  45331.600018    40.927143   4444.369695     0.098403   \n",
      "std     577.494589  14326.327119    13.262450   3045.410024     0.057620   \n",
      "min       1.000000  20014.489470    18.055189      1.377630     0.000049   \n",
      "25%     500.750000  32796.459717    29.062492   1939.708847     0.047903   \n",
      "50%    1000.500000  45789.117313    41.382673   3974.719419     0.099437   \n",
      "75%    1500.250000  57791.281668    52.596993   6432.410625     0.147585   \n",
      "max    2000.000000  69995.685578    63.971796  13766.051239     0.199938   \n",
      "\n",
      "           default  \n",
      "count  2000.000000  \n",
      "mean      0.141500  \n",
      "std       0.348624  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max       1.000000  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../LogisticRegression/credit_data.csv\")\n",
    "print(data.describe())\n",
    "\n",
    "features = data[[\"income\",\"age\",\"loan\"]]\n",
    "target = data.default\n",
    " \n",
    "X_data = np.array(features).reshape(-1,3)\n",
    "y_data = np.array(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE HAVE TO USE PREPROCESSING AND APPLY MIN_MAX TRANFORMATION ON THE FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are usually transformed into a range before the kNN algorihtm is applied.<br>\n",
    "This is because the distance formula depends on how features are measured.<br>\n",
    "If cerntain features have a much larger value than others then the ditance measurements will be strongly dominated by the large values.<br>\n",
    "We have to rescale the feautures such that each one contributes equally to the distance formula.<br>\n",
    "For this we use--<br>\n",
    "&nbsp;&nbsp;&nbsp;1.&nbsp; **min-max normalization**<br>\n",
    "&nbsp;&nbsp;&nbsp;2.&nbsp; **z-score normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min-max normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process transforms features such that all its values lie between 0 and 1.<br>\n",
    "Normalized values can be interpreted as how far the each of those values fall between the **minima** and **maxima**.<br>\n",
    "<br>\n",
    "$X_{new} = \\frac{X \\ - \\ min(X)}{max(X) \\ - \\ min(X)}$<br><br>\n",
    "We would like to use min-max normalization on the income features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  z-score normalization (standardization) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm uses mean and standard deviation to normalize the data.<br><br>\n",
    "$X_{new} = \\frac{X \\ - \\ X_{mean}}{StandardDeviation(X)}$<br><br>\n",
    "These values are not within 0-1.<br> Used for principle component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-data before normalization---\n",
      "[[6.61559251e+04 5.90170151e+01 8.10653213e+03]\n",
      " [3.44151540e+04 4.81171531e+01 6.56474502e+03]\n",
      " [5.73171701e+04 6.31080495e+01 8.02095330e+03]\n",
      " ...\n",
      " [4.43114493e+04 2.80171669e+01 5.52278669e+03]\n",
      " [4.37560566e+04 6.39717958e+01 1.62272260e+03]\n",
      " [6.94365796e+04 5.61526170e+01 7.37883360e+03]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X-data before normalization---\")\n",
    "print(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-data after normalization---\n",
      "[[0.9231759  0.89209175 0.58883739]\n",
      " [0.28812165 0.65470788 0.47682695]\n",
      " [0.74633429 0.9811888  0.58262011]\n",
      " ...\n",
      " [0.48612202 0.21695807 0.40112895]\n",
      " [0.47500998 1.         0.1177903 ]\n",
      " [0.98881367 0.82970913 0.53597028]]\n"
     ]
    }
   ],
   "source": [
    "X_data = preprocessing.MinMaxScaler().fit_transform(X_data)\n",
    "print(\"X-data after normalization---\")\n",
    "print(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test-split\n",
    "features_train,features_test,target_train,target_test = train_test_split(X_data,y_data,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's run the classifier on our data\n",
    "model = KNeighborsClassifier(n_neighbors=30)\n",
    "fitted_model = model.fit(features_train,target_train)\n",
    "predictions = fitted_model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix -- \n",
      "[[503   1]\n",
      " [ 17  79]]\n",
      "\n",
      "Accuracy score is : 0.97\n"
     ]
    }
   ],
   "source": [
    "#Let's see our prediction results\n",
    "print(\"Confusion Matrix -- \")\n",
    "print(confusion_matrix(target_test,predictions))\n",
    "print()\n",
    "print(f\"Accuracy score is : {accuracy_score(target_test,predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This score outperforms Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Folds Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the knn classification 99 times with diffrent values for k.<br>\n",
    "For each iteration a value for k is used to classify the data.<br>\n",
    "Using 10 folds the score for each fold for a value of k is calculated.<br>\n",
    "We take the mean score for each k and append it to an array.<br>\n",
    "Calculate the max of this array which gives us the optimal value of k!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate optimal k-value\n",
    "cross_valid_scores = []\n",
    "\n",
    "for k in range(1, 100):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_data, y_data, cv=10, scoring='accuracy')\n",
    "    cross_valid_scores.append(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k with cross-validation: 28 and the accuracy score is is 0.9845122628065702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Optimal k with cross-validation: {np.argmax(cross_valid_scores)} and the accuracy score is is {max(cross_valid_scores)}.\")\n",
    "optimal_k = np.argmax(cross_valid_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[[503   1]\n",
      " [ 17  79]]\n",
      "0.97\n"
     ]
    }
   ],
   "source": [
    "model_CV = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "fitted_model_CV = model.fit(features_train, target_train)\n",
    "predictions_CV = fitted_model_CV.predict(features_test)\n",
    "print(confusion_matrix(target_test, predictions_CV))\n",
    "print(accuracy_score(target_test, predictions_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the score above Cross Validation was not used(but we used optimal k) so the score is 1.4% lower**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">END!</span> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
