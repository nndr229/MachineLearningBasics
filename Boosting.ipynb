{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Can be used for both regression and classification.</li>        \n",
    "    <li>Helps reduce variance and bias.</li>\n",
    "    <br>\n",
    "    <li>In <strong>Bagging</strong> the algorithm creates  multiple copies of the original data and constructs several decision trees on the copies and combines all the trees to make predictions.</li>\n",
    "    <ul>\n",
    "        <strong>\n",
    "            <li>\n",
    "                WE CONSTRUCT THESE TREES INDEPENDENTLY!!!\n",
    "            </li>\n",
    "        </strong>\n",
    "    </ul>\n",
    "    <br>\n",
    "    <li>In <strong>Boosting</strong> the decision trees are grown sequentially (sequential algorithm). Each tree is grown using information from previously grown trees.\n",
    "    <ul>\n",
    "        <strong>\n",
    "            <li>\n",
    "                THE TREES ARE NOT INDEPENDENT FROM EACH OTHER!!!\n",
    "            </li>\n",
    "        </strong>\n",
    "    </ul>  \n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "It is based on a rather counter-intuitive theory: A weak learner is not able to make good predictions. <br>\n",
    "<ul>\n",
    "    <li>A weak learner is just a little better someone who guesses randomly (like coin flip). For example: Decision Trees with depth 1.</li>\n",
    "    <li>Combining weak learners can prove to be an extremely powerful classifier !!!</li>\n",
    "    <li style=\"color:red;font-weight:bold\">By fitting small trees (<strong>decision stumps</strong>) we slowly improve the final result in case it doesn't perform well.\n",
    "        <ul>\n",
    "            <li>We use adaptive boosting. <strong style=\"color:green\">Adaboost algorithm!!!</strong></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>Used in OpenCV: Viola-Jones face detection algorithm.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting illustration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Boosting</strong> combines very simple weak learners such as <strong>decision trees</strong> with depth <strong>1</strong> (capable of linear classification).<br>\n",
    "<img src = \"./Images/boosting1.png\"/>\n",
    "<p>1. We want to end with with a seperation of the green dots and yellow dots.</p>\n",
    "<img src= \"https://github.com/nndr229/MachineLearningBasics/blob/main/Images/boosting2.png\" style=\"width:300px;\"/>\n",
    "<p>2. The classifier made 2 mistakes as 2 yellow dots are misclassified.</p>\n",
    "<p>3. In the next iteration it will focus on the misclassified items.\n",
    "    <ul>\n",
    "        <li>A weight <strong>w</strong> has been already assigned to every single sample in the dataset.</li>\n",
    "        <li>Increase the weights <strong>w</strong> for the misclassified items.</li>\n",
    "        <li>Decrease the weights <strong>w</strong> for the correctly classified items.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>We can't use a straight line as this is not a linearly seperable problem.</p>\n",
    "<img src = \"boosting3.png\" style=\"width:300px\"/>\n",
    "<p>4. In the next iteration the algorithm correctly classifies a yellow dot that was previously misclassified.</p>\n",
    "<p>5. In the next iteration the algorithm will focus on the last misclassified dot.</p>\n",
    "<img src = \"Images/boosting4.png\" style=\"width:300px\"/>\n",
    "<p style=\"color:green;font-weight:bold\">The algorihtm has come up with 3 weak but simple decision tree learners to classify this dataset accurately (a non-linear decision boundary has been found sequentially).WE JUST HAVE TO COMBINE THESE CLASSIFIERS !!!</p>\n",
    "<img src = \"Images/boosting5.png\" style=\"width:300px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Equations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Images/equation.png\" style=\"width:300px\"/>\n",
    "<p style=\"text-align:center;font-weight:bold\">Sign Function</p>\n",
    "<h4 style=\"\">$H(x) \\  =  \\ sign \\sum_{i=1}^n \\alpha_i \\ h_i(x)$</h4>\n",
    "<ul style=\"text-align:center;line-width:10px\"><br>\n",
    "    <li style=\"text-align:left;\">Final model $H(x)$ is a strong classifier.</li><br>\n",
    "    <li style=\"text-align:left\">We keep combining $h_i(x)$ :weak learners that only know to calssify part of the full space.</li><br>\n",
    "     <li style=\"text-align:left\">We use $\\alpha_i$ parameters to control the importance of each $h_i(x)$ .</li><br>\n",
    "    <li style=\"text-align:left\">We assign $+1$ and $-1$ for the output classes(yellow and green).</li><br>\n",
    "    <li style=\"text-align:left\">We initialize the weight parameters $w_i = \\frac{1}{N}$  for every single sample in the dataset.</li><br>\n",
    "        <li style=\"text-align:left\">$\\sum_{i=1}^n$ : We make sure this is a distribution.</li><br>\n",
    "    <li style=\"text-align:left\">$\\epsilon = \\sum_{wrong}w_i$ : error is the sum of the misclassified weights.</li><br>\n",
    "</ul>\n",
    "\n",
    "<strong>STEPS</strong>\n",
    "<ol>\n",
    "    <li>Initialize the weight parameters: <span style=\"color:red;font-size:15px\">$w_i = \\frac{1}{N}$</span> </li><br>\n",
    "    <li>Pick $h_t(x)$ that minimizes $ \\ \\epsilon \\ $ error term: <span style=\"color:red;font-size:15px\"> $\\epsilon_t = \\sum_{wrong}w_i$</span></li><br>\n",
    "    <li>Calculate $a_t$: <span style=\"color:red;font-size:15px\">$a_t = \\frac{1}{2}\\ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$</span></li><br>\n",
    "    <li>Update $w_{t+1}$: <span style=\"color:red;font-size:15px\">$w_i^{t+1} = \\frac{w_i^t}{Z} e^{-\\alpha_t h_t(x) y(x)}$</span></li><br>\n",
    "    <li>Go back to Step 2 and repeat.</li><br>\n",
    "<ol>\n",
    "<p>In every iteration we add a new $h(x)$ or weak learner to the final model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\alpha$ parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Images/alpha.png\" style=\"width:300px\"/>\n",
    "<p style=\"text-align:center;font-size:20px\">$a_t = \\frac{1}{2}\\ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$</p>\n",
    "<ul>\n",
    "    <li>Given an $h(x)$ calssifier the $\\alpha$ value increases as the error converges to 0 (good classifiers are given more weight.).</li>\n",
    "    <li>Given an $h(x)$ calssifier the $\\alpha$ value is 0 if the error is 0.5. Beacause it is a random guess like a coin toss(we do not want to rely on random guesses).</li>\n",
    "    <li>We give a negative $\\alpha$ value for $h(x)$ calssifiers that are worse than random guesses meaning the error rate is greater than 0.5 .</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $w$ parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;font-size:20px\">$w_i^{t+1} = \\frac{w_i^t}{Z} e^{-\\alpha_t h_t(x) y(x)}$</p>\n",
    "<ul>\n",
    "    <li>We use the equation to update weights in every iteration.</li>\n",
    "    <li>we use higher weights for for more important samples and vice versa.</li>\n",
    "    <li>The $Z$ makes sure $w$ is a distribution so that the sum is 1.</li>\n",
    "    <li>$y(x)$ flips the sign of the exponent if $h(x)$ is wrong. This is good as it assigns higher weights to misclassified samples and smaller weights for correctly classified samples.\n",
    "    <ul>\n",
    "        <li>In the next iteration the $h(x)$ learner can focus on those samples with higher weights.</li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>Why use $\\alpha$ in the formula? We use $\\alpha$ value to make sure that a classifier that predicts more correctly become more imporatant.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">CODE (IRIS DATASET)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "\n",
    "features = iris_data.data\n",
    "targets = iris_data.target\n",
    "\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, targets, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use Ada-Boost\n",
    "\n",
    "#We usually set the base estimator as a decision tree.\n",
    "\n",
    "# n_estimators: Here: the number of decision trees.\n",
    "#               The maximum number of estimators at which boosting is terminated. \n",
    "#               In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "#learning_rate: Learning rate shrinks the contribution of each classifier by learning_rate. \n",
    "#               There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "#random_state: Controls the random seed given at each base_estimator at each boosting iteration.\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100, learning_rate=1, random_state=123)\n",
    "model.fitted = model.fit(feature_train, target_train)\n",
    "model.predictions = model.fitted.predict(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is : \n",
      "[[11  0  0]\n",
      " [ 0 10  1]\n",
      " [ 0  0  8]]\n",
      "Accuracy score is : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix is : \")\n",
    "print(confusion_matrix(target_test, model.predictions))\n",
    "print(f\"Accuracy score is : {accuracy_score(target_test, model.predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">CODE (WINE DATASET)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tasty(quality):\n",
    "    if quality >= 7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data = pd.read_csv(\"../../PythonMachineLearning_HolczerBalazs/Datasets/Datasets/wine.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_wine = data[\n",
    "    [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\",\n",
    "     \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]]\n",
    "data['tasty'] = data[\"quality\"].apply(is_tasty)\n",
    "targets_wine = data['tasty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density   pH  sulphates  alcohol  \n",
      "0                 45.0                 170.0    1.001  3.0       0.45      8.8  \n"
     ]
    }
   ],
   "source": [
    "#our features\n",
    "print(features_wine[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "4893    0\n",
      "4894    0\n",
      "4895    0\n",
      "4896    1\n",
      "4897    0\n",
      "Name: tasty, Length: 4898, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#our targets\n",
    "print(targets_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features_wine).reshape(-1, 11)\n",
    "y = np.array(targets_wine)\n",
    "# X = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "#When we use preprocessing we find that our accuracy reduces.\n",
    "#We need minMaxScaler for kNN . For boosting we should avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_w, feature_test_w, target_train_w, target_test_w = train_test_split(features_wine, targets_wine, test_size=.2)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.3, 1],\n",
    "}\n",
    "\n",
    "estimator = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=estimator, param_grid=param_dist, cv=10)\n",
    "grid_search.fit(feature_train_w, target_train_w)\n",
    "\n",
    "predictions_wine = grid_search.predict(feature_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is : \n",
      "[[743  45]\n",
      " [111  81]]\n",
      "Accuracy score is : 0.8408163265306122\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix is : \")\n",
    "print(confusion_matrix(target_test_w, predictions_wine))\n",
    "print(f\"Accuracy score is : {accuracy_score(target_test_w, predictions_wine)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:14px;font-weight:bold\">Bagging is preffered over Boosting for decision trees as it reduces variance and solves over-fitting. Boosting reduces bias but has some tendency to over-fit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">END</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
