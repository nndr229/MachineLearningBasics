{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Supervised learning algorithm mostly used in classification but can be used for regression as well as they work well for both categorical(like sunny,cloudy,rainy) and continuous output(numercial values like 87.2342,234.234).</li>\n",
    "    <li>Very similar to search trees.</li>\n",
    "    <li>Splits the data into two or more homogeneous sets based on splitter in input variables.</li>\n",
    " </ul>\n",
    " <h4>We have a single feature <strong>x</strong>, the number of hours a student spends studying. We want to predict the probability <strong>y</strong> that he/she will pass an exam.</h4>\n",
    "<ul>\n",
    "    <li><strong>Categorical Variable Decision Tree:</strong> variable such as fail/pass or yes/no.</li>\n",
    "    <li><strong>Continuous Variable Decision Tree:</strong> continuous variables like the ones used in regression.</li>\n",
    "</ul>\n",
    "<img src = \"Images/categoricaldt.png\" style=\"width:600px;\"/>\n",
    "<ul>\n",
    "    <li><strong style=\"color:green\">ROOT NODE : </strong>Represents the entire dataset/population and this further gets divided into several subsets (The root node corresponds to the best predictor).</li>\n",
    "    <li><strong style=\"color:#ECCC15\">LEAF NODES : </strong>Nodes that have no children. These are the values that we are after! Our output(s)!</li>\n",
    "    <li><strong style=\"color:steelblue\">DECISION NODES : </strong>The algorithm spkits the node into Sub-nodes based on the given dataset.</li>\n",
    "</ul>\n",
    "<img src =\"Images/dt.png\" style=\"width:500px\"/>\n",
    "<p>The decision tree classifier depends heavily on splits.<br>There are several algorithms for this problem.<br>\n",
    "<ul>\n",
    "    <li>Gini Index Approach.</li>\n",
    "    <li>Calculating the information entropy(ID3 algorithm or C4.5 approach).</li>\n",
    "    <li>Algorithm based in variance reduction.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Categorical Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Image from Holczer Balazs's course on ML(UDEMY)</p>\n",
    "<p>DATSET PREDICTING WHETHER TO PLAY GOLF OR NOT BASED ON WAETHER.</p>\n",
    "<img src=\"Images/cdt.png\">\n",
    "<p>We have to decide what is going to be the root node? What are the branches ? and so on...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 algorithm and Shannon Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Usually <strong>ID3</strong> algorithm is used to build the decision tree ~ it is a top down greedy search of all possible branches.<br>\n",
    "It uses <strong>entropy</strong> and <strong>information gain</strong> to build the tree.\n",
    "</p>\n",
    "The $H(x)$ , Shannon entropy of a discrete random variable $X$ with possible values $x_1,x_2,...,x_n$ and probability mass function $P(x)$ is defined as : <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $H(X) = -\\sum_{i}^n P(x_i)\\log_2P(x_i)$<br><br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\">Go here for more info about Shannon Entropy (See Example)</a>.<br>\n",
    "<p style=\"font-weight:bold;color:red\">A BRANCH WITH ENTROPY MORE THAN 1 REQUIRES SPLITTING!!!<br>\n",
    "    <li style=\"font-weight:bold;color:red\"> Root node has the maximum information gain(entropy reduciton).</li>\n",
    "    <li style=\"font-weight:bold;color:red\"> Leaf nodes have entropy 0.</li>\n",
    "</p>\n",
    "<p>Consider the golf dataset.</p>\n",
    "<p>Playing Golf:\n",
    "    <li>9 Times YES</li>\n",
    "    <li>5 Times NO</li>\n",
    "    <li>$H(Playing  \\ Golf) = H(9,5) = -(0.64 \\log_2{0.64}) -(0.36 \\log_2{0.36}) \\  = \\ 0.94 $</li>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have to be able  to calculate the entropy wrt a given feature to be able to calculate information gain.** <br>\n",
    "$E(T,X) = \\sum_{x} P(x)E(x)$<br>\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <td style=\"font-weight:bold; background:steelblue;color:white\"></td>\n",
    "            <td style=\"font-weight:bold; background:steelblue;color:white\">YES</td>\n",
    "            <td style=\"font-weight:bold;  background:steelblue;color:white\">NO</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "          <td ></td>\n",
    "          <td style=\"font-weight:bold;background:steelblue;color:white\">Sunny</td>\n",
    "          <td>2</td>\n",
    "          <td>3</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td style=\"font-weight:bold\">OUTLOOK</td>\n",
    "          <td style=\"font-weight:bold;background:steelblue;color:white\">Overcast</td>\n",
    "          <td>4</td>\n",
    "          <td>0</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "          <td ></td>\n",
    "          <td style=\"font-weight:bold;background:steelblue;color:white\">Rainy</td>\n",
    "          <td>3</td>\n",
    "          <td>2</td>\n",
    "        </tr>  \n",
    "    </tbody>\n",
    "</table>\n",
    "<p style=\"text-align:center;font-weight:bold\"><em>E(PlayGolf,Outlook)</em> = P(Sunny)E(2,3) +  P(Overcast)E(4,0) +  P(Rainy)E(3,2)</p>\n",
    "<p style=\"text-align:center;font-weight:bold\"> = $\\frac{5}{14}*0.971 + \\ \\frac{4}{14}*0 + \\ \\frac{5}{14}*0.971  \\ \\ = \\ \\ 0.6936$</p>\n",
    "\n",
    "<h3>What about information gain?</h3>\n",
    "<p>It is the decrease in entropy after a dataset is split on an attribute/feature.</p>\n",
    "<li>Feature/attribute with the highest information gain will be the root node.</li>\n",
    "<p style=\"font-weight:bold;color:red\"> Information Gain(Outlook) = H(Play Golf) - E(Play Golf, Outlook) = 0.94 - 0.693 = \"0.247\" (HIGHEST)</p>\n",
    "<p style=\"font-weight:bold;color:green\"> Information Gain(Temperature) = H(Play Golf) - E(Play Golf, Temperature)= \"0.029\"</p>\n",
    "<p style=\"font-weight:bold;color:green\"> Information Gain(Humidity) = H(Play Golf) - E(Play Golf, Humidity)= \"0.152\"</p>\n",
    "<p style=\"font-weight:bold;color:green\"> Information Gain(Wind) = H(Play Golf) - E(Play Golf, Wind)= \"0.048\"</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Index Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>CART</strong>(Classification and Regression Tree) algorithm uses Gini index to decide how to make splits.<br>\n",
    "<ul>\n",
    "    <li>Gini Index approach is a little bit better than calculating the information gain and entropy.(THE ALGORITHMS ARE APPROXIMATELY THE SAME!!!)</li>\n",
    "    <li>When dealing with entropy we have to calculate logarithms ,which is computationally expensive. So we prefer Gini index approach.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p>Formula used to calculate the gini index of an <strong>X</strong> random variable.</p><br>\n",
    "$G(X) = 1 - \\sum_{i=1}^n {P(x_i)^2} \\ \\ $ Where n is the number of output classes<br><br>\n",
    "\n",
    "$E(s,t) = G(t) -  P(left)G({left}_t) - P(right)G(right_t) $<br><br>\n",
    "<strong style=\"color:red\">We have to calculate this E value to know which features to split.</strong>\n",
    "<strong style=\"color:red\">We split the feature with the lowest Gini Index.</strong>\n",
    "<p>Consider the golf dataset again.</p>\n",
    "<p>Playing Golf:\n",
    "    <li>9 Times YES</li>\n",
    "    <li>5 Times NO</li>\n",
    "    <li><strong>G</strong>$(Playing \\ Golf) = 1 - (0.64)^2 - (0.36)^2 = 0.46 $</li>\n",
    "</p>\n",
    "<p>Think that we are dealing with the credit scoring dataset. So the <strong>target variable is WILL DEFAULT / WILL NOT DEFAULT.</strong></p>\n",
    "$E(s,t) = G(t) -  P(left)G({left}_t) - P(right)G(right_t) $<br><br>\n",
    "<strong>Where</strong>\n",
    "<ul>\n",
    "    <li>$G(t)$ is the Gini Index for node t.</li>\n",
    "    <li>$P(left)G({left}_t)$ is the proportion/probability of observation in the left node after splitting times Gini index of the left node after splitting.</li>\n",
    "    <li>$P(right)G(right_t)$ is the proportion/probability of observation in the right node after splitting times Gini index of the right node after splitting.</li>\n",
    "</ul>\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <td style=\"font-weight:bold; background:steelblue;color:white\"></td>\n",
    "            <td style=\"font-weight:bold; background:steelblue;color:white\">YES</td>\n",
    "            <td style=\"font-weight:bold;  background:steelblue;color:white\">NO</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "          <td style=\"font-weight:bold;\">GENDER</td>\n",
    "          <td style=\"font-weight:bold;background:steelblue;color:white\">Male</td>\n",
    "          <td>3</td>\n",
    "          <td>2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td style=\"font-weight:bold\"></td>\n",
    "          <td style=\"font-weight:bold;background:steelblue;color:white\">Female</td>\n",
    "          <td>2</td>\n",
    "          <td>0</td>\n",
    "        </tr>   \n",
    "    </tbody>\n",
    "</table>\n",
    "<p> $G(t) =  1 - (\\frac{7}{9})^2 - (\\frac{2}{9})^2 \\ \\ = 0.3432$</p>\n",
    "<p> $G(male) = 1 - (\\frac{3}{5})^2 - (\\frac{2}{5})^2 \\ \\ = 0.48$</p>\n",
    "<p> $G(female) = 1 - (\\frac{4}{4})^2 - (\\frac{0}{5})^2 \\ \\ = 0$</p>\n",
    "<p> $E(s,t) = 0.34 - \\frac{5}{9}*0.48 - \\frac{4}{9}*0 \\ \\ = 0.07$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Discision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color:green\">Advantages</strong>\n",
    "<ul>\n",
    "<li>One of the best approaches to identify most significant variables and the relationship between them.</li>\n",
    "<li>No need for data-preprocessing.Not influenced by outliers.</li>\n",
    "<li>Can handle numberical and categorical variables.</li>\n",
    "</ul>\n",
    "<strong style=\"color:red\">Disadvantages</strong>\n",
    "<ul>\n",
    "<li>Have a tendency to overfit because we solve it by pruning.</li>\n",
    "<li>Unstable because a small variation in the data might result in a completely different trre being generated.Generating the optimal tree like structure is NP-complete , so we use a greedy approach.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">CODE for Decision Tree On Iris Dataset</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "\n",
    "features = iris_data.data\n",
    "targets = iris_data.target\n",
    "\n",
    "#train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, targets, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted mean score(across 10 folds) for our decision tree our classifier 0.96\n"
     ]
    }
   ],
   "source": [
    "#criterion passed as a parameter to the clsasifier can something like gini or entropy. Default is gini.\n",
    "model = DecisionTreeClassifier(criterion=\"gini\")\n",
    "\n",
    "#we use cross-validation\n",
    "predicted = cross_validate(model, features, targets, cv=10)\n",
    "print(f\"Predicted mean score(across 10 folds) for our decision tree our classifier {np.mean(predicted['test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tree Constructed by scikit learn using Entropy model.</h2>\n",
    "<img src = \"Images/outtree2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">Parameter Tuning for Decision Tree On Iris Dataset</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter with Grid Search:  {'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "#parameter grid with values from 1 - 9\n",
    "param_grid = {'max_depth': np.arange(1, 10)}\n",
    "\n",
    "tuned_tree = GridSearchCV(DecisionTreeClassifier(), param_grid,cv=3,iid=False)\n",
    "\n",
    "tuned_tree.fit(feature_train, target_train)\n",
    "\n",
    "print(\"Best parameter with Grid Search: \", tuned_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[[12  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  0  7]]\n",
      "\n",
      "Accuracy score: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#make predicitons\n",
    "grid_predictions = tuned_tree.predict(feature_test)\n",
    "\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(target_test, grid_predictions))\n",
    "print()\n",
    "print(\"Accuracy score: \")\n",
    "print(accuracy_score(target_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We could achieve an accuracy of 100% with parameter tuning!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">Identifying Cancer with Decision Trees</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict whether malignant with 0 or benign with 1\n",
    "#There are 30 features in the dataset\n",
    "\n",
    "cancer_data = datasets.load_breast_cancer()\n",
    "\n",
    "features_cancer = cancer_data.data\n",
    "labels_cancer = cancer_data.target\n",
    "\n",
    "feature_train_c, feature_test_c, target_train_c, target_test_c = train_test_split(features_cancer, labels_cancer, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9105760089879871\n"
     ]
    }
   ],
   "source": [
    "model_cancer_gini = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "\n",
    "predicted_c_gini = cross_validate(model_cancer_gini, features_cancer, labels_cancer, cv=10)\n",
    "print(np.mean(predicted_c_gini['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9439104226082448\n"
     ]
    }
   ],
   "source": [
    "model_cancer_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "\n",
    "predicted_c_entropy = cross_validate(model_cancer_entropy, features_cancer, labels_cancer, cv=10)\n",
    "print(np.mean(predicted_c_entropy['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that we got a better accuracy score using the \"entropy model\" for this dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">END</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
