{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Big Problem with Decision Trees!!!</h3>\n",
    "<p>Every split it makes at each node is optimazed for the dataset it is fit to.<br>\n",
    "This splitting process will rarely generalise well to other/new data.<br>\n",
    "The algorithm overfits!!!\n",
    "</p>\n",
    "<p style=\"color:red; font-weight:bold\">Generally it is impossible to train a model that trains to fit a training dataset and is capable of predicting given a new test dataset.<br>\n",
    "WE WANT A MODEL THAT CAPTURES THE RELATIONSHIPS WITHIN THE TRAINING DATSET AND GENRALIZES WELL TO UNSEEN DATA.<br>    \n",
    "This is called bias-variance trade-off.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>BIAS</strong>\n",
    "<ul>\n",
    "    <li>Error from reclassifications in the learning algorithm.</li>\n",
    "    <li>High Bias : The algorithm misses relevant information between features and targets (<em>underfitting</em>).</li>\n",
    "    <li><strong>ERROR DUE TO MODEL MISMATCH!</strong></li>\n",
    "</ul>\n",
    "<strong>VARIANCE</strong>\n",
    "<ul>\n",
    "    <li>Error in sensitivity to small changes in training set.</li>\n",
    "    <li>High Variance: The algorithm models noise (<em>overfitting</em>).</li>\n",
    "    <li><strong>VARAINCE DUE TO TRAINING SAMPLE AND RANDOMIZATION.</strong></li>\n",
    "</ul>\n",
    "<strong style=\"color:red\">BIAS / VARIANCE Trade-Off</strong>\n",
    "<ul>\n",
    "    <li>We are not able to optimize both variance and bias.</li>\n",
    "    <li>Low bias -> High variance</li>\n",
    "    <li>Low variance -> Hish bias</li>\n",
    "</ul>\n",
    "    <img src = \"Images/bias_variance_to.png\"/>\n",
    "    <p style=\"text-align:center\"><em>Image from -> <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Usually decision trees are likely to overfit the data leading to poor test performance.<br>\n",
    "Trees are also unstable classifiers. If you perturb the data a little the tree might change significantly (low bias but high variance model).</p>\n",
    "<p>Smaller Tree + Fewer Splits : Better predictor at the cost of a little extra bias.<br>\n",
    "    <strong style=\"color:green\">Better solution : grow a large tree and then prune it back to a smaller subtree.</strong>\n",
    "</p>\n",
    "<img src = \"Images/bfpruning.png\" style=\"width:450px\"/>\n",
    "<img src = \"Images/afpruning.png\" style=\"width:450px\"/>\n",
    "<p style=\"text-align:center\"><em>Image from Holczer Balazs's ML course on Udemy.</em></p>\n",
    "<p>But we prefer other mehods like Bagging and Random Forest Classifier as pruning introduces a little extra bias.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Bagging stands for <strong>Bootstrap aggregation.</strong></p>\n",
    "A rather counter-intuitive theory: A weak learner is not able to make good predictions. <br>\n",
    "<ul>\n",
    "    <li>A weak learner is just a little better someone who guesses randomly (like coin flip). For example: Decision Trees with depth 1.</li>\n",
    "    <li>Combining weak learners can prove to be an extremely powerful classifier !!!</li>\n",
    "    <li>Black-Scholes model is approximately the same: two risky positions taken together can effectively eliminate the risk itself.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li>Bagging reduces variance of a learning algorithm.</li>\n",
    "    <li>If we have a set $X$ of $n$ independent variables $x_1,x_2,...,x_n$ each with variance $V$ then the variance of the mean $X$ is $\\frac{V}{n}$.<br>\n",
    "        <strong style=\"color:red\">We can reduce the variance by averaging a set of observations.</strong>\n",
    "    </li><br>\n",
    "    <li><strong>The idea: </strong> Have multiple training sets and construct a decision tree (without pruning) on every single dataset !!!</li>\n",
    "    <li><strong>Problem: </strong>We do not have several training sets.</li>\n",
    "    <li><strong style=\"color:green\">Solution: </strong>We should take repeated samples from the single dataset, then construct trees and finally average all predicitons at the end.<ul><li>All trees are fullt grown unpruned decision trees.</li></ul></li>\n",
    "    <li><strong>Regression Problem: </strong>We take the average.</li>\n",
    "    <li><strong>Classification Problem: </strong>We take the majority vote.</li>\n",
    "</ul> \n",
    "\n",
    "<strong>DISADVANTAGES</strong>\n",
    "<ul>\n",
    "    <li>The contructed trees are highly correlated because we use all the features.</li>\n",
    "    <li>Every dataset has a strong predictor/feature amongst all the features. All the bagged trees tend to make the same splits as they all share the same features !!! <br>\n",
    "        <strong>Because of this all the trees look similar.</strong></li>\n",
    "    <li>We can do better.</li>\n",
    "</ul>\n",
    "<img src = \"Images/bagging.png\"/>\n",
    "<p style=\"text-align:center\"><em>Image from Holczer Balazs's ML course on Udemy.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This algorithm <strong>decorrelates</strong> those single decision trees that were constructed.</li>\n",
    "    <li>This reduces variance even more when averaging trees.</li>\n",
    "    <li>Similar to bagging:<strong style=\"color:red\"> We keep contructing decision trees on the training data. But on every split in the tree, a random selection of features / predictors is chosen from the full feature set.</strong><br><br>\n",
    "        <ul>\n",
    "            <li>The number of features considered at a given split is approximately equal to the square root of the total number of features.</li>\n",
    "            <li><strong>Bagging: </strong>algorithm searches over all $N$ features to find the best feature that splits the data at that node.</li>\n",
    "            <li><strong>Random Forest Classifier: </strong>algorithm searches over random $\\sqrt{N}$ features to find the best one.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<img src = \"Images/RFC.png\" style=\"width:700px\"/>\n",
    "<p style=\"text-align:center\"><em>Image from Holczer Balazs's ML course on Udemy.</em></p><br>\n",
    "<strong style=\"color:green\">Why is this good?</strong>\n",
    "<ul>\n",
    "    <li>If one of the features are very strong predictors for the response variable (TARGET), these features will be selected in many of the decision trees, so only they will be correlated.</li>\n",
    "    <li>At some point the variance starts decreasing no matter how many more trees we add to our random forest. So there won't be overfitting!!!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">CODE (IRIS DATASET)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "\n",
    "features = iris_data.data\n",
    "targets = iris_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train, feature_test, target_train, target_test = train_test_split(features, targets, test_size=.2)\n",
    "\n",
    "#max_features: The number of features to consider when looking for the best split.\n",
    "#n_estimators: The number of trees in the forest.\n",
    "model = RandomForestClassifier(n_estimators=1000, max_features='sqrt')\n",
    "\n",
    "fitted_model = model.fit(feature_train, target_train)\n",
    "predictions = fitted_model.predict(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Iris dataset : \n",
      "[[11  0  0]\n",
      " [ 0  5  0]\n",
      " [ 0  1 13]]\n",
      "\n",
      "Accuracy score for Iris dataset : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Confusion matrix for Iris dataset : \" + \"\\n\"\n",
    "      f\"{confusion_matrix(target_test, predictions)}\")\n",
    "print()\n",
    "print(f\"Accuracy score for Iris dataset : {accuracy_score(target_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">CODE (CREDIT DATASET)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression accuracy: 93%\n",
    "# we do better with knn: 97.5% !!!\n",
    "# 84% simple kNN without normalizing the dataset\n",
    "# we can achieve ~ 99% with random forests\n",
    "\n",
    "credit_data = pd.read_csv(\"../LogisticRegression/credit_data.csv\")\n",
    "\n",
    "features_credit = credit_data[[\"income\", \"age\", \"loan\"]]\n",
    "targets_credit = credit_data.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy over 10 folds is : 0.9894923373084328\n"
     ]
    }
   ],
   "source": [
    "X = np.array(features_credit).reshape(-1, 3)\n",
    "y = np.array(targets_credit)\n",
    "\n",
    "model_credit = RandomForestClassifier(n_estimators=100)\n",
    "predicted = cross_validate(model_credit, X, y, cv=10)\n",
    "\n",
    "print(f\"Mean accuracy over 10 folds is : {np.mean(predicted['test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">CODE (DIGIT DATASET and PARALLEL COMPUTING)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_data = datasets.load_digits()\n",
    "\n",
    "image_features = digit_data.images.reshape((len(digit_data.images), -1))\n",
    "image_targets = digit_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_jobs :The number of jobs to run in parallel. \n",
    "#fit, predict, decision_path and apply are all parallelized over the trees. \n",
    "#-1 means using all processors. \n",
    "random_forest_model = RandomForestClassifier(n_jobs=-1, max_features='sqrt')\n",
    "\n",
    "feature_train_digit, feature_test_digit, target_train_digit, target_test_digit = train_test_split(image_features, image_targets, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are : \n",
      "{'max_depth': 15, 'min_samples_leaf': 1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#min_samples_leaf : The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 100, 500, 1000],\n",
    "    \"max_depth\": [1, 5, 10, 15],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 10, 15, 30, 50]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=10,iid=False)\n",
    "grid_search.fit(feature_train_digit, target_train_digit)\n",
    "print(\"Best parameters are : \")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal n_estimators: 100\n",
      "Optimal optimal_depth: 15\n",
      "Optimal optimal_leaf: 1\n"
     ]
    }
   ],
   "source": [
    "optimal_estimators = grid_search.best_params_.get(\"n_estimators\")\n",
    "optimal_depth = grid_search.best_params_.get(\"max_depth\")\n",
    "optimal_leaf = grid_search.best_params_.get(\"min_samples_leaf\")\n",
    "\n",
    "print(f\"Optimal n_estimators: {optimal_estimators}\")\n",
    "print(f\"Optimal optimal_depth: {optimal_depth}\")\n",
    "print(f\"Optimal optimal_leaf: {optimal_leaf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for the digit dataset is : \n",
      "[[32  0  0  0  2  0  0  0  0  0]\n",
      " [ 0 29  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 39  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 42  0  0  0  1  2  0]\n",
      " [ 0  0  0  0 35  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 32  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 30  0  0]\n",
      " [ 0  1  1  0  0  0  0  0 38  0]\n",
      " [ 0  0  0  1  0  0  0  0  0 36]]\n",
      "\n",
      "Accuracy score for digit dataset is : 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid_search.predict(feature_test_digit)\n",
    "print(\"Confusion matrix for the digit dataset is : \")\n",
    "print(confusion_matrix(target_test_digit, grid_predictions))\n",
    "print()\n",
    "print(f\"Accuracy score for digit dataset is : {accuracy_score(target_test_digit, grid_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the algorithm a few more times we can get an accuracy of approximately 99.5%!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">END</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
